---
title: <span style="color:#034a94"> **Análisis de Conglomerados**</span>
author: "Modelos Estadísticos para la toma de decisiones"
output: html_document
css: style.css
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(paqueteMOD)

```


</br></br>

# <span style="color:#034a94">**Introducción**</span>

</br>

El análisis de conglomerados, también conocido como **clustering**, es un método estadístico usado para agrupar objetos similares en función de sus características. Mediante este análisis se logra identificar grupos muy parecidos (homogéneos) de objetos o individuos. Dentro de cada grupo los objetos son más similares entre sí que con los de otros grupos. El análisis de conglomerados implica la selección de un conjunto de variables para medir las características de los objetos o individuos, y luego aplicar un algoritmo de agrupamiento para clasificarlos en conglomerados.  Los algoritmos de agrupamiento utilizados pueden ser jerárquicos o no jerárquicos, dependiendo de si los grupos se construyen de forma iterativa a partir de subgrupos más pequeños.

</br></br>

# <span style="color:#034a94">**Supuestos y requisitos**</span>

</br>

El análisis de conglomerados se basa en calcular distancias utilizando las componentes principales calculadas mediante ACP, estas distancias serán equivalentes a calcularlas mediante la matriz original $X$, una vez encontrados los conglomerados o grupos se procede a representar el primer plano factorial, por lo que los supuestos del análisis de conglomerados serán los mismos del análisis de componentes principales.

</br></br>

## <span style="color:#034a94">**Modelo**</span>

</br>

Existen dos tipos de clasificación automática: los métodos no jerárquicos los cuales se basan en encontrar la mejor partición del conjunto de individuos en $q$ clases, en donde sus centros de gravedad se eligen en un inicio de forma aleatoria. Por otra parte están los métodos jerárquicos en los cuales se construye un dendograma en el cual se forman los grupos de individuos más parecidos, esto permite determinar el número de clases que se usarán en el método no jerárquico. 

</br>

Mediante el análisis de conglomerados se desea clasificar a los distintos individuos u observaciones en grupos muy homogéneos, pero heterogeneos entre ellos. Para realizar este objetivo se utilizan métricas que perminten calcular el grado de asociación (similitud o disimilitud) entre dos observaciones, dentro de las  más usada  está la **distancia euclidiana** entre los dos puntos, la cual está dado por:

</br>

$$
d(x, y) = \sqrt{\sum_{i=1}^{p} \Big(x_i - y_i\Big)^2}
$$

</br>

En donde $x$ y $y$ son individuos con $p$ variables, de esta forma podremos obtener los dos primeros individuos que más se parezcan entre sí, formarán el primer grupo. Sean entonces $h = \{x, y\}$ un grupo y $z$ un individuo, con los que se puede calcular la distancia entre el grupo y el individuo de la forma: $d(h, z) = min\{d(x,z), d(y, z)\}$. Además hay otras formas de medir la proximidad entre elementos como pueden ser el salto mínimo (single linkage), salto máximo, salto promedio y agregación de Ward. <br>

El siguiente dendograma fue construido a apartir de 5 individuos:

<center>
```{r, echo=FALSE, out.width="80%", fig.align = "center"}
knitr::include_graphics("img/dendograma.png")
```

</center>

</br></br>

En este diagrama se observa que al inicio los individuos que más se parecen (menor distancia euclidea) son los individuos ($1$) y ($3$) los cuales tuvieron una distancia de $1$. El siguiente grupo a formarse lo conforman  los individuos ($2$) y ($5$) con una distancia de $2$, el siguiente nodo a formarse está conformado por  el grupo {$1, 3$} y el individuo ($4$) con una distancia de $4$. 

</br>

Elegir el número óptimo de clusters o grupos  es una decisión subjetiva, sin embargo puede tomarse el criterio del mayor salto de nodo a nodo de las distancias euclidianas: Observando el dendograma vemos que el mayor incremento de las distancias se dio en $4$, por lo que si trazamos una linea se hará un corte y tendremos dos nodos, el conformado por {$1,3,4$} y el otro con {$2,5$}. 

</br>

Otro método de agrupamiento es el $k-means$ (o centros moviles), este se centra en dividir un conjunto de datos en $k$ grupos. En este se trata de minimizar la suma de los cuadrados de las distancias entre los puntos y su centroide correspondiente, los pasos en este algoritmo son:

* 1) Selecciona $k$ centroides aleatorios de los puntos en el conjunto de datos.

* 2) Asigna cada punto al centroide más cercano.

* 3) Recalcula los centroides como la media de los puntos asignados a cada centroide.

* 4) Repite los pasos $2$ y $3$ hasta que los centroides no cambien significativamente o se alcance un número máximo de iteraciones (es decir, el algoritmo converja).

* 5) El número k de clusters se debe especificar antes de ejecutar el algoritmo.


</br></br>

### <span style="color:#FF7F00">**Ejemplo**</span> 

</br>

El siguiente ejemplo  en **R** cuenta con  un conjunto de datos sobre $35$ observaciones de `whiskys`, que contiene las variable `precio`, `proporción de alcohol`, `años de añejo` y la `nota` que le da el jurado a la calidad de estos.

</br></br>

```{r message = FALSE}
data = read.table("whisky.txt", header = TRUE, row.names = 1)
head(data)
```

</br></br>

Mediante la libreria *factoextra* podemos calcular las distancias euclidianas de los individuos y gráficar el dendograma del conjunto de datos:

</br>

```{r message = FALSE}
library(factoextra)
distancias <- dist(data, method = "euclidean")
dendograma <- hclust(distancias, method = "average")
plot(dendograma, cex = 0.6, hang = -1) 
barplot(sort(dendograma$height, decreasing = TRUE), horiz = TRUE, 
        main = "Agregaciones (distancias euclidianas)",
        col = "lightblue", ylab = "Nodo", xlab = "Peso", xlim = c(0, 80))
```

</br></br>

Podemos observar que el mayor salto en las agregaciones se da en 60.38, por lo que se puede realizar un corte con este valor en el dendograma, quedando:

</br>

```{r eval= FALSE}
fviz_dend(dendograma, k=2, cex = 0.5,
          k_colors = c("red",  "blue"), 
          color_labels_by_k = TRUE, 
          rect=TRUE)
```


```{r, echo=FALSE, out.width="80%", fig.align = "center"}
knitr::include_graphics("img/dendograma2.png")
```

</br>

En este caso el número optimo de grupos o clusters será de 2. Con el siguiente código podemos ver en la primera fila los individuos y en la segunda los clusters a los que estos pertenecen.


</br></br>

```{r}
grp <- cutree(dendograma, k = 2)
grp
```


</br></br>

La libreria `FactoClass` para representar los clusters en el primer plano factorial, en la función `FactoClass` tenemos los siguientes argumentos: 

</br></br>

* `nf`: número de componentes principales, recordemos que en el acp este será igual al número de variables. <br>

* `nfcl`: número de componentes usadas para clasificación, en este caso será igual a` nf=p`. <br>

* `k.clust`: número de clusters, en este caso mediante el dendograma elegimos dos clusters. 

</br></br>

```{r message = FALSE}
library(FactoClass)
p = ncol(data)
Cluster <- FactoClass(data, dudi.pca, nf = p, nfcl = p, k.clust = 2, scanFC = FALSE)
```

</br></br>

```{r,eval=FALSE}
plotFactoClass(Cluster,title="Análisis de conglomerados")
```

</br></br>

```{r, echo=FALSE, out.width="80%", fig.align = "center"}
knitr::include_graphics("img/cluster1.png")
```

</br></br>

<!-- A partir de ahora podemos realizar el análisis que hicimos anteriormente en un ACP, podemos observar que en el primer plano factorial se representa el $75.99$\% de la varianza total, quedando un $24.01$\% sin explicar mediante las dos primeras componentes. Se evidencia que el cluster 2 se caracteriza por tener porcentaje altos de alcohol, altos precios y mayores años de añejo, por otra parte el cluster 1 se caracteriza por tener precios mucho más economicos a costa de tener menor porcentaje de alcohol y pocos años de añejo. -->










