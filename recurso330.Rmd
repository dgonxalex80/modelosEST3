---
title: <span style="color:#034a94"> **Evaluación del modelo**</span>
author: "Modelos Estadísticos para la toma de decisiones"
output: html_document
css: style.css
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(comment = NA, warning = FALSE, message = FALSE)
# colores
c1="#FF7F00"
c2="#=EB0C6"
c3="#034A94"
c4="#686868"
color2=c(c1,c2)
library(memisc)
library(MASS)
library(lattice)
library(stats)
library(tidyverse)

# install.packages("learnr")          # solo una vez
# install.packages("devtools")     # solo una vez
#devtools::install_github("dgonxalex80/paqueteMOD", force = TRUE) #descarga paquete nivelatorioEST
library(paqueteMOD)
data("dataMat")
dataMat = sample(dataMat, 1000, replace = TRUE)
glm(gana ~ nota , family = binomial(link = "logit"), data = dataMat) -> modelo1
summary(modelo1) 
```

 </br></br>

## <span style="color:#034A94">**Bondad de ajuste del modelo**</span>

</br>

Para determinar la bondad de ajuste del modelo se utiliza el resultado de la probabilidad que estima el modelo y se asigna un valor de cero para los individuos que tengan valores menores o iguales a un punto $c$, que en este caso tomamos como $0.50$. Para valores mayores a $0.5$ se asigna un valor de $1$. Al construir una tabla con los resultados de la predicción contra los valores reales de $Y$, obtenemos:

</br>


<pre>
        |                predicción    |  
        |         |   No    |    Si    |
--------|---------|---------|----------|   
Estado  |  No     |     723 |      37  |
real    |  Si     |      53 |     187  |

</pre>

</br></br>

Representados por el siguiente gráfico de mosaico:

</br>


```{r, echo=FALSE}
library(vcd)
predicciones <- ifelse(test = modelo1$fitted.values > 0.5, "Si" , "No")
mc <- table(modelo1$model$gana, predicciones,
                          dnn = c("Estado real", "Predicciones"))
mosaic(mc, shade = T, colorize = T,
       gp = gpar(fill = matrix(c("#11224D", "#F98125", "#F5B841","#2C599D"), 2, 2)))
```

</br></br>

Los colores azules representan la proporción de clasificaciones correcta :

</br>

* Siendo $No$, lo clasifica como $No$ : **VN** 

* Siendo $Si$ lo clasifica como $Si$  : **VP**

</br>

Los colores naranjas corresponden a las proporciones de clasificaciones erradas por el modelo.

* Siendo $Si$, lo clasifica incorrectamente como $No$ :  Falsos Negativo : **FN**  

* Siendo $No$, lo  clasifica incorrectamente como $Si$ : Falso Positovo  : **FP**

</br></br>

La proporción de clasificaciones correctas dan una aproximación del valor $R^2$

</br>

$$ 
\dfrac{723+187}{723+53+37+187} = \dfrac{910}{1000} = 0.910
$$
Este valor cuenta como el $R^2$ , es decir que el modelo explica (clasifica de manera adecuada) el 91% de los casos.

</br></br>


Este valor se puede obtener de la matriz de confusión que se obtiene con una data que no se ha empleado en la estimación del modelo

</br></br>

## <span style="color:#034A94">**Matriz de confusión**</span>

</br>

Consiste en un método de evaluación del modelo estimado, mediante la separación de la data en dos partes. Una primera para estimar el modelo (train) que puede corresponder entre el 60% y el 80% de los datos y el restante porcentaje para una muestra con la que se evalúa el poder de predicción del modelo (Test)

Lo primero será estimar el modelo con la data.train y posteriormente valuar el modelo utilizando la data.test

Con los resulados obtenidos por la predicción del modelo sobre la muestra.test se construye la matriz de confisión que tiene la siguiente forma:

</br></br>

```{r, echo=FALSE, out.width="80%", fig.align = "center"}
knitr::include_graphics("img/matriz_confusion.png")
```


</br></br>


```{r, echo=FALSE}
# names(dataMat)
# nrow(dataMat)

ntrain <- nrow(dataMat)*0.6
ntest <- nrow(dataMat)*0.4
c(ntrain,ntest)

set.seed(123)
index_train<-sample(1:nrow(dataMat),size = ntrain)
train<-dataMat[index_train,]  # muestra de entrenamiento
test<-dataMat[-index_train,]  # muestra de prueba
```

</br></br>

```{r}
library(tidyverse)
valor_pronosticado <- predict(modelo1,test,type = "response")
niveles_pronosticados <- ifelse(valor_pronosticado >0.5, "Si","No") %>%
                             factor(.)

rendimiento_data<-data.frame(observados=test$gana,
                             predicciones= niveles_pronosticados)

Positivos <- sum(rendimiento_data$observados=="Si")
Negativos <- sum(rendimiento_data$observados=="No")
Positivos_pronosticados <- sum(rendimiento_data$predicciones=="Si")
Negativos_pronosticados <- sum(rendimiento_data$predicciones=="No")
Total <- nrow(rendimiento_data)
VP<-sum(rendimiento_data$observados=="Si" & rendimiento_data$predicciones=="Si")
VN<-sum(rendimiento_data$observados=="No" & rendimiento_data$predicciones=="No")
FP<-sum(rendimiento_data$observados=="No" & rendimiento_data$predicciones=="Si")
FN<-sum(rendimiento_data$observados=="Si" & rendimiento_data$predicciones=="No")

matriz_confusion=matrix(c(VP, FP, FN,VN), nrow=2)

rownames(matriz_confusion) = c(" No ", " Si    ")
colnames(matriz_confusion) = c("No", "Si")
# matriz_confusion
```


</br></br>

<pre>
        |                predicción             |  
        |         |   Si       |    No          |
--------|---------|------------|----------------|   
Estado  |  Si     |   81  (VP) |     22   (FN)  |
real    |  No     |   13  (FP) |     284  (VN)  |
</pre>


</br>

>

A partir de la matriz de confusión se obtienen indicadores 

</br></br>

## <span style="color:#034a94">**Capacidad de clasificación del modelo**</span>

</br>

En una de las características más importantes del modelo, pues permite valorar a través de la matriz de confusión las clasificaciones correctas por medio de las predicciones del modelo. 

Existen adicionamente otros indicadores que pueden ser utilizados como es el estadistico c asociado a la curva ROC (Trvrivrt Oprtsyong Charasteristic) . Esta curva compara diferentes puntos de corte de la probabilidad que permite establecer la tasa de clasificaciones correcta de verdaderos positivos y de falsos positivos:

</br></br>


### <span style="color:#FF7F00"> **Exactitud**</span>

Porcentaje de casos correctamente clasificados

<div class="content-box-blue">
$$
\text{Exactitud} = \dfrac{\text{aciertos en clasificación}}{\text{Total}} = \dfrac{VP + VN}{\text{Total}} = \dfrac{81+281}{400} = 0.905
$$
</div>

</br></br>

### <span style="color:#FF7F00"> **Tasa de error**</span>


Porcentaje de casos incorrectamente clasificados

<div class="content-box-blue">
$$
\text{Exactitud} = \dfrac{\text{no aciertos en clasificación}}{\text{Total}} = \dfrac{FP + FN}{\text{Total}} = \dfrac{13+22}{400} = 0.0875
$$

</br></br>

### <span style="color:#FF7F00"> **Sensibilidad**</span>

Porcentaje de positivos que son clasificados por el modelo como positivos 

<div class="content-box-blue">
$$
\text{Sensibilidad} = \dfrac{VP}{\text{Total positivos}} = \dfrac{81}{103} = 0.786
$$
</div>
</br></br>

### <span style="color:#FF7F00"> **Especificidad**</span>

Porcentaje de negativos que son clasificados por el modelo como negativos 

<div class="content-box-blue">
$$
\text{Especificidad} = \dfrac{\text{VN}}{\text{Total negativos}} = \dfrac{284}{297} = 0.956
$$
</div>

</br></br>

### <span style="color:#FF7F00"> **Presición**</span> 

Porcentaje de positivos pronosticados de manera correcta


<div class="content-box-blue">
$$
\text{Precision} = \dfrac{\text{VP}}{\text{Total clasificados positivos}} = \dfrac{81}{94} = 0.862 
$$ 
</div>

</br></br>

### <span style="color:#FF7F00"> **Valor de predicción negativo**</span> 

Porcentaje de negativos clasificados por el modelo como negativos


<div class="content-box-blue">
$$
\text{Valor de prediccion negativo} = \dfrac{\text{VN}}{\text{Total clasificados negativos}} = \dfrac{284}{306} = 0.928 
$$ 
</div>

</br></br>


### <span style="color:#686868"> **Resumen**</span>

</br>

```{r, eval=FALSE}
library(tidyverse)
Exactitud <- (VP+VN)/Total
Tasa_de_Error <- (FP+FN)/Total
Sensibilidad <- VP/Positivos
Especificidad <- VN/Negativos
Precision <- VP/Positivos_pronosticados
Valor_prediccion_negativo <- VN / Negativos_pronosticados

indicadores <- t(data.frame(Exactitud,Tasa_de_Error,Sensibilidad,Especificidad,Precision,Valor_prediccion_negativo))
indicadores %>% 
  round(.,3)
```
<pre>
                           
Exactitud                  0.912

Tasa de Error              0.088

Sensibilidad               0.786

Especificidad              0.956

Precisión                  0.862

Valor predicción negativo  0.928

</pre>

</br></br>

<div class="content-box-yellow">
### <span style="color:#686868">**Nota:**</span> 

* Interesa tener valores de 

|                            |                     |
|:--------------------------:|:-------------------:|
| Exactitud                  | alto                |
| Tasa de Error              | bajo                |
| Sensibilidad               | alto                |
| Especificidad              | alto                |
| Precisión                  | alto                |
| Valor predicción negativo  | alto                |

</br>

* El valor **c** de corte de la probabilidad, incide sobre los indicadores 

</br>

* El balanceo de los porcentajes de la variable dependiente es importante y afecta a los indicadores 

</div>



</br></br>



### **Curva ROC**

</br>

```{r}
library(pROC)
curva_ROC <- roc(test$gana, valor_pronosticado)
auc<- round(auc(curva_ROC, levels =c(0,1), direction = "<"),4) # 0.9177

ggroc(curva_ROC, colour = "#FF7F00", size=2)+
ggtitle(paste0("Curva ROC ", "(AUC = ", auc, ")"))+
xlab("Especificidad")+
ylab("Sensibilidad")  
```

El área comprendida entre la curva ROC y la diagonal del cuadrado  (AUC = 0.9234), indica un buen ajuste del modelo de predicción. 


